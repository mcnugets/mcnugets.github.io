<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>EDROPCNN</title>
        <link href="css/styles.css" rel="stylesheet" />

        

    </head>
    <body id="page-top" class="fw-light" style="text-align: justify;">
            <div class="container px-4 text-center mt-3">
                <h2>Effectiveness of different dimensionality reduction techniques on pruned deep neural network</h2>
                <a href="https://www.linkedin.com/in/sultangazy-yergaliyev-2aa956231/">Sultangazy Yergaliyev
                

            </div>

            <div class="container  px-4 text-center mt-3">
                <a class="btn btn-lg btn-light" href="https://github.com/mcnugets/Effectiveness-of-different-dimensionality-reduction-techniques-on-pruned-deep-neural-network">
                    GitHub</a>
                <a class="btn btn-lg btn-light" href="https://mcnugets.github.io/thesis/01.pdf">
                    Paper</a>

            </div>
            <div class="container px-4 mt-5 ">
                <div class="row gx-4 justify-content-center ">
                    <div class="col-lg-8 ">
                        <h3>Abstract</h3>
                        <p>
                            In machine learning, throughout the years the architecture of neural networks has been under
                            rigorous observation and has been developed constantly. However, when it comes to the
                            complexity of the data and the structure of layers within the network our best strategy is to
                            come up with the best architecture so that when we use the network for any reoccurring
                            problem, heavy computational consumption can be avoided. Hence pruning is proposed by
                            many papers that explore the field of pruning on how to find an effective subnetwork that has
                            just as much efficiency as the original network. Furthermore, there are many optimization
                            techniques besides pruning, one of which is dimensional reduction, which is a projection of
                            multivariate data into a lower dimension, for effectivity purposes. In this paper, we propose
                            dimensional reduction methods like Principal component analysis(PCA), Independent
                            Component Analysis(ICA), and Isometric mapping(Isomap), and utilize these methods for
                            weight matrices decomposition within the pre-trained network that were trained using datasets
                            like CIFAR10, and MNIST. We use those methods in two scenarios: one is where the network
                            is unpruned, and two where we prune the network and then apply dimensionality reduction
                            afterwards. Moreover, we develop a technique that combines dimensional reduction and matrix
                            transformation that helps us to preserve as much information as possible. We believe that this
                            experiment provides interesting results with some potential.
                        </p>
                    </div>
                </div>
            </div>
       
        <!-- Overview -->
            <div class="container px-4">
                <div class="row gx-4 justify-content-center">
                    <div class="col-lg-8">
                        <h3>Overview</h3>
                        <p>
                            The project focused on the use of dimensional reduction techniques and applying them to the pruned
                            convolutional neural network. This aims to achieve an improved optimization via extracting the sparse weight matrices(after pruning) of a 
                            particular network, hence solving the problem of network size and computation time. As opposed to training a relatively smaller
                            network, the goal is to avoid unnecessary computations and preserve as much information as possible 
                            after performing dimensional reduction.
                        </p>
                    </div>
                </div>
            </div>
        
        <!-- Footer-->
        <footer class="py-5 bg-dark">
            <div class="container px-4"><p class="m-0 text-center text-white">Copyright &copy; Your Website 2023</p></div>
        </footer>
        <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
    </body>
</html>
